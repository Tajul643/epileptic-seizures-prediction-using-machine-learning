# -*- coding: utf-8 -*-
"""tajul.py

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1TCt7bAHdGIFQqziVSraUGlc1MTLKUM-7

**Epileptic_Seizures implementation by the Tajul Islam Ayon**
"""

import numpy as np 
import pandas as pd 
import matplotlib.pyplot as plt 
import seaborn as sns

from google.colab import files
uploaded = files.upload()

df = pd.read_csv('ep.csv') 
df.head(10)

df.shape

df.info

df.isna().sum()

df = df.dropna(axis=1)

df.shape

df['y'].value_counts()

#Visualize this count 
sns.countplot(df['y'],label="Count")

df.describe()

df.shape

sns.countplot(df['y'])
plt.ylabel(')Total numbers of 0 to 4 values')
plt.xlabel('Actual numbers in the database')

df.loc[df["y"]==2,"y"]=1
df.loc[df["y"]==3,"y"]=1
df.loc[df["y"]==4,"y"]=1
df['y'].value_counts()

df.shape

df.size

df.dtypes

def dataset_description(df):
    columns=df.columns.to_list()
    print('No. of Columns in DataFrame: ',len(columns))
    print('\nColumn attribute Names: ',columns)
    ncol=df.describe().columns.to_list()
    ccol=[]
    for i in columns:
        if(ncol.count(i)==0):
            ccol.append(i)
        else:
            continue
    print('\nNo. of Numerical Column in DataFrame: ',len(ncol))
    print('\nNumerical Column Names: ',ncol)
    print('\nNo. of Categorical Column in DataFrame: ',len(ccol))
    print('\nCategorical Column Names: ',ccol)

df.isnull().sum()

df.nunique()

df.describe().T

df.corr()

df.skew()

plt.figure(figsize=(14,14))
plt.subplot(3,2,1)
plt.style.use('seaborn')
plt.tight_layout()
sns.set_context('talk')
sns.histplot(data=df, x='X1', hue="y",multiple="stack",palette='magma')
plt.title('X1	 vs y')

plt.subplot(3,2,2)
plt.style.use('seaborn')
plt.tight_layout()
sns.set_context('talk')
sns.histplot(data=df, x='X4', hue="y",multiple="stack",palette='magma')
plt.title('X4 vs y')

plt.subplot(3,2,3)
plt.style.use('seaborn')
plt.tight_layout()
sns.set_context('talk')
sns.histplot(data=df, x='X5', hue="y",multiple="stack",palette='magma')
plt.title('X5	 vs y')

plt.subplot(3,2,4)
plt.style.use('seaborn')
plt.tight_layout()
sns.set_context('talk')
sns.histplot(data=df, x='X6', hue="y",multiple="stack",palette='magma')
plt.title('X6 vs y')

plt.subplot(3,2,5)
plt.style.use('seaborn')
plt.tight_layout()
sns.set_context('talk')
sns.histplot(data=df, x='X8', hue="y",multiple="stack",palette='magma')
plt.title('X8	 vs y')

plt.subplot(3,2,6)
plt.style.use('seaborn')
plt.tight_layout()
sns.set_context('talk')
sns.histplot(data=df, x='X177', hue="y",multiple="stack",palette='magma')
plt.title('X177 vs y')
plt.show()

import seaborn as sns
corrmat = df.corr()
top_corr_features = corrmat.index
plt.figure(figsize=(20,20))
sns.heatmap(df[top_corr_features].corr(),annot=True,cmap='RdYlGn')
plt.show()

df.hist(layout = (17, 12),
            figsize = (10, 30),
            color=['green'])

sns.pairplot(df, hue="diagnosis")

def OneHotEncoding(dfcolumn):
  global df
  dfcolumn.nunique()
  len(df.columns)
  finallencol = (dfcolumn.nunique() - 1) + (len(df.columns)-1)
  dummies = pd.get_dummies(dfcolumn, drop_first=True, prefix=dfcolumn.name)
  df=pd.concat([df,dummies],axis='columns')
  df.drop(columns=dfcolumn.name,axis=1,inplace=True) 
  if(finallencol==len(df.columns)):
    print('OneHotEncoding is sucessfull') 
    print('')
  else:
    print('Unsucessfull')
  return df.head(5)

OneHotEncoding(df['X3'])
OneHotEncoding(df['X1'])
OneHotEncoding(df['X5'])
OneHotEncoding(df['X8'])
OneHotEncoding(df['X177'])

df.describe().columns.to_list()

df = df[['X2',
 'X4',
 'X6',
 'X7',
 'X9',
 'X10',
 'X11',
 'X12',
 'X13',
 'X14',
 'X15',
 'X16',
 'X17',
 'X18',
 'X19',
 'X20',
 'X21',
 'X22',
 'X23',
 'X24',
 'X25',
 'X26',
 'X27',
 'X28',
 'X29',
 'X30',
 'X31',
 'X32',
 'X33',
 'X34',
 'X35',
 'X36',
 'X37',
 'X38',
 'X39',
 'X40',
 'X41',
 'X42',
 'X43',
 'X44',
 'X45',
 'X46',
 'X47',
 'X48',
 'X49',
 'X50',
 'X51',
 'X52',
 'X53',
 'X54',
 'X55',
 'X56',
 'X57',
 'X58',
 'X59',
 'X60',
 'X61',
 'X62',
 'X63',
 'X64',
 'X65',
 'X66',
 'X67',
 'X68',
 'X69',
 'X70',
 'X71',
 'X72',
 'X73',
 'X74',
 'X75',
 'X76',
 'X77',
 'X78',
 'X79',
 'X80',
 'X81',
 'X82',
 'X83',
 'X84',
 'X85',
 'X86',
 'X87',
 'X88',
 'X89',
 'X90',
 'X91',
 'X92',
 'X93',
 'X94',
 'X95',
 'X96',
 'X97',
 'X98',
 'X99',
 'X100',
 'X101',
 'X102',
 'X103',
 'X104',
 'X105',
 'X106',
 'X107',
 'X108',
 'X109',
 'X110',
 'X111',
 'X112',
 'X113',
 'X114',
 'X115',
 'X116',
 'X117',
 'X118',
 'X119',
 'X120',
 'X121',
 'X122',
 'X123',
 'X124',
 'X125',
 'X126',
 'X127',
 'X128',
 'X129',
 'X130',
 'X131',
 'X132',
 'X133',
 'X134',
 'X135',
 'X136',
 'X137',
 'X138',
 'X139',
 'X140',
 'X141',
 'X142',
 'X143',
 'X144',
 'X145',
 'X146',
 'X147',
 'X148',
 'X149',
 'X150',
 'X151',
 'X152',
 'X153',
 'X154',
 'X155',
 'X156',
 'X157',
 'X158',
 'X159',
 'X160',
 'X161',
 'X162',
 'X163',
 'X164',
 'X165',
 'X166',
 'X167',
 'X168',
 'X169',
 'X170',
 'X171',
 'X172',
 'X173',
 'X174',
 'X175',
 'X176',
 'X178',
 'y',
 'X3_-1741',
 'X3_-1681',
 'X3_-1564',
 'X3_-1516',
 'X3_-1492',
 'X3_-1406',
 'X3_-1254',
 'X3_-1244',
 'X3_-1210',
 'X3_-1182',
 'X3_-1176',
 'X3_-1149',
 'X3_-1131',
 'X3_-1110',
 'X3_-1107',
 'X3_-1091',
 'X3_-1090',
 'X3_-1089',
 'X3_-1080',
 'X3_-1034',
 'X3_-1002',
 'X3_-990',
 'X3_-983',
 'X3_-981',
 'X3_-980',
 'X3_-970',
 'X3_-968',
 'X3_-966',
 'X3_-950',
 'X3_-923',
 'X3_-914',
 'X3_-911',
 'X3_-903',
 'X3_-891',
 'X3_-890',
 'X3_-883',
 'X3_-872',
 'X3_-871',
 'X3_-864',
 'X3_-859',
 'X3_-856',
 'X3_-846',
 'X3_-837',
 'X3_-824',
 'X3_-823',
 'X3_-818',
 'X3_-816',
 'X3_-809',
 'X3_-800',
 'X3_-784',
 'X3_-783',
 'X3_-771',
 'X3_-770',
 'X3_-762',
 'X3_-755',
 'X3_-745',
 'X3_-740',
 'X3_-734',
 'X3_-731',
 'X3_-730',
 'X3_-725',
 'X3_-718',
 'X3_-717',
 'X3_-716',
 'X3_-713',
 'X3_-711',
 'X3_-710',
 'X3_-704',
 'X3_-697',
 'X3_-694',
 'X3_-686',
 'X3_-674',
 'X3_-668',
 'X3_-665',
 'X3_-663',
 'X3_-652',
 'X3_-649',
 'X3_-645',
 'X3_-644',
 'X3_-643',
 'X3_-642',
 'X3_-641',
 'X3_-640',
 'X3_-627',
 'X3_-622',
 'X3_-621',
 'X3_-618',
 'X3_-614',
 'X3_-613',
 'X3_-607',
 'X3_-599',
 'X3_-598',
 'X3_-597',
 'X3_-596',
 'X3_-594',
 'X3_-592',
 'X3_-590',
 'X3_-588',
 'X3_-587',
 'X3_-585',
 'X3_-582',
 'X3_-576',
 'X3_-575',
 'X3_-569',
 'X3_-568',
 'X3_-567',
 'X3_-560',
 'X3_-554',
 'X3_-553',
 'X3_-548',
 'X3_-546',
 'X3_-544',
 'X3_-541',
 'X3_-536',
 'X3_-535',
 'X3_-530',
 'X3_-529',
 'X3_-527',
 'X3_-525',
 'X3_-524',
 'X3_-523',
 'X3_-522',
 'X3_-521',
 'X3_-518',
 'X3_-516',
 'X3_-514',
 'X3_-513',
 'X3_-512',
 'X3_-511',
 'X3_-510',
 'X3_-509',
 'X3_-506',
 'X3_-504',
 'X3_-502',
 'X3_-501',
 'X3_-500',
 'X3_-496',
 'X3_-494',
 'X3_-491',
 'X3_-489',
 'X3_-488',
 'X3_-483',
 'X3_-481',
 'X3_-477',
 'X3_-475',
 'X3_-473',
 'X3_-471',
 'X3_-470',
 'X3_-469',
 'X3_-468',
 'X3_-466',
 'X3_-464',
 'X3_-462',
 'X3_-461',
 'X3_-459',
 'X3_-456',
 'X3_-453',
 'X3_-452',
 'X3_-449',
 'X3_-448',
 'X3_-447',
 'X3_-446',
 'X3_-445',
 'X3_-443',
 'X3_-442',
 'X3_-439',
 'X3_-433',
 'X3_-432',
 'X3_-427',
 'X3_-426',
 'X3_-423',
 'X3_-422',
 'X3_-421',
 'X3_-419',
 'X3_-418',
 'X3_-417',
 'X3_-415',
 'X3_-414',
 'X3_-411',
 'X3_-407',
 'X3_-405',
 'X3_-401',
 'X3_-400',
 'X3_-399',
 'X3_-398',
 'X3_-397',
 'X3_-396',
 'X3_-394',
 'X3_-393',
 'X3_-392',
 'X3_-391',
 'X3_-388',
 'X3_-387',
 'X3_-386',
 'X3_-385',
 'X3_-383',
 'X3_-382',
 'X3_-381',
 'X3_-380',
 'X3_-377',
 'X3_-376',
 'X3_-375',
 'X3_-374',
 'X3_-372',
 'X3_-371',
 'X3_-370',
 'X3_-369',
 'X3_-366',
 'X3_-365',
 'X3_-364',
 'X3_-363',
 'X3_-361',
 'X3_-360',
 'X3_-359',
 'X3_-358',
 'X3_-357',
 'X3_-356',
 'X3_-355',
 'X3_-354',
 'X3_-353',
 'X3_-351',
 'X3_-350',
 'X3_-349',
 'X3_-348',
 'X3_-347',
 'X3_-346',
 'X3_-344',
 'X3_-343',
 'X3_-341',
 'X3_-340',
 'X3_-338',
 'X3_-337',
 'X3_-336',
 'X3_-335',
 'X3_-334',
 'X3_-333',
 'X3_-332',
 'X3_-331',
 'X3_-330',
 'X3_-329',
 'X3_-328',
 'X3_-327',
 'X3_-326',
 'X3_-325',
 'X3_-323',
 'X3_-322',
 'X3_-321',
 'X3_-320',
 'X3_-319',
 'X3_-316',
 'X3_-314',
 'X3_-312',
 'X3_-310',
 'X3_-309',
 'X3_-308',
 'X3_-307',
 'X3_-306',
 'X3_-305',
 'X3_-304',
 'X3_-302',
 'X3_-301',
 'X3_-300',
 'X3_-299',
 'X3_-298',
 'X3_-297',
 'X3_-295',
 'X3_-294',
 'X3_-293',
 'X3_-292',
 'X3_-291',
 'X3_-289',
 'X3_-288',
 'X3_-287',
 'X3_-286',
 'X3_-285',
 'X3_-284',
 'X3_-283',
 'X3_-282',
 'X3_-281',
 'X3_-280',
 'X3_-279',
 'X3_-278',
 'X3_-277',
 'X3_-276',
 'X3_-275',
 'X3_-274',
 'X3_-273',
 'X3_-272',
 'X3_-271',
 'X3_-270',
 'X3_-269',
 'X3_-268',
 'X3_-267',
 'X3_-266',
 'X3_-265',
 'X3_-264',
 'X3_-263',
 'X3_-262',
 'X3_-261',
 'X3_-260',
 'X3_-259',
 'X3_-258',
 'X3_-257',
 'X3_-256',
 'X3_-255',
 'X3_-254',
 'X3_-253',
 'X3_-252',
 'X3_-251',
 'X3_-250',
 'X3_-248',
 'X3_-247',
 'X3_-246',
 'X3_-245',
 'X3_-244',
 'X3_-243',
 'X3_-242',
 'X3_-241',
 'X3_-240',
 'X3_-239',
 'X3_-238',
 'X3_-237',
 'X3_-236',
 'X3_-235',
 'X3_-234',
 'X3_-233',
 'X3_-232',
 'X3_-231',
 'X3_-230',
 'X3_-229',
 'X3_-228',
 'X3_-227',
 'X3_-226',
 'X3_-225',
 'X3_-224',
 'X3_-223',
 'X3_-221',
 'X3_-219',
 'X3_-218',
 'X3_-217',
 'X3_-216',
 'X3_-215',
 'X3_-214',
 'X3_-213',
 'X3_-212',
 'X3_-211',
 'X3_-210',
 'X3_-209',
 'X3_-208',
 'X3_-207',
 'X3_-206',
 'X3_-205',
 'X3_-204',
 'X3_-203',
 'X3_-202',
 'X3_-201',
 'X3_-200',
 'X3_-199',
 'X3_-198',
 'X3_-197',
 'X3_-196',
 'X3_-195',
 'X3_-194',
 'X3_-193',
 'X3_-192',
 'X3_-191',
 'X3_-190',
 'X3_-189',
 'X3_-188',
 'X3_-187',
 'X3_-186',
 'X3_-185',
 'X3_-184',
 'X3_-183',
 'X3_-182',
 'X3_-181',
 'X3_-180',
 'X3_-179',
 'X3_-178',
 'X3_-177',
 'X3_-176',
 'X3_-175',
 'X3_-174',
 'X3_-173',
 'X3_-172',
 'X3_-171',
 'X3_-170',
 'X3_-169',
 'X3_-168',
 'X3_-167',
 'X3_-166',
 'X3_-165',
 'X3_-164',
 'X3_-163',
 'X3_-162',
 'X3_-161',
 'X3_-160',
 'X3_-159',
 'X3_-158',
 'X3_-157',
 'X3_-156',
 'X3_-155',
 'X3_-154',
 'X3_-153',
 'X3_-152',
 'X3_-151',
 'X3_-150',
 'X3_-149',
 'X3_-148',
 'X3_-147',
 'X3_-146',
 'X3_-145',
 'X3_-144',
 'X3_-143',
 'X3_-142',
 'X3_-141',
 'X3_-140',
 'X3_-139',
 'X3_-138',
 'X3_-137',
 'X3_-136',
 'X3_-135',
 'X3_-134',
 'X3_-133',
 'X3_-132',
 'X3_-131',
 'X3_-130',
 'X3_-129',
 'X3_-128',
 'X3_-127',
 'X3_-126',
 'X3_-125',
 'X3_-124',
 'X3_-123',
 'X3_-122',
 'X3_-121',
 'X3_-120',
 'X3_-119',
 'X3_-118',
 'X3_-117',
 'X3_-116',
 'X3_-115',
 'X3_-114',
 'X3_-113',
 'X3_-112',
 'X3_-111',
 'X3_-110',
 'X3_-109',
 'X3_-108',
 'X3_-107',
 'X3_-106',
 'X3_-105',
 'X3_-104',
 'X3_-103',
 'X3_-102',
 'X3_-101',
 'X3_-100',
 'X3_-99',
 'X3_-98',
 'X3_-97',
 'X3_-96',
 'X3_-95',
 'X3_-94',
 'X3_-93',
 'X3_-92',
 'X3_-91',
 'X3_-90',
 'X3_-89',
 'X3_-88',
 'X3_-87',
 'X3_-86',
 'X3_-85',
 'X3_-84',
 'X3_-83',
 'X3_-82',
 'X3_-81',
 'X3_-80',
 'X3_-79',
 'X3_-78',
 'X3_-77',
 'X3_-76',
 'X3_-75',
 'X3_-74',
 'X3_-73',
 'X3_-72',
 'X3_-71',
 'X3_-70',
 'X3_-69',
 'X3_-68',
 'X3_-67',
 'X3_-66',
 'X3_-65',
 'X3_-64',
 'X3_-63',
 'X3_-62',
 'X3_-61',
 'X3_-60',
 'X3_-59',
 'X3_-58',
 'X3_-57',
 'X3_-56',
 'X3_-55',
 'X3_-54',
 'X3_-53',
 'X3_-52',
 'X3_-51',
 'X3_-50',
 'X3_-49',
 'X3_-48',
 'X3_-47',
 'X3_-46',
 'X3_-45',
 'X3_-44',
 'X3_-43',
 'X3_-42',
 'X3_-41',
 'X3_-40',
 'X3_-39',
 'X3_-38',
 'X3_-37',
 'X3_-36',
 'X3_-35',
 'X3_-34',
 'X3_-33',
 'X3_-32',
 'X3_-31',
 'X3_-30',
 'X3_-29',
 'X3_-28',
 'X3_-27',
 'X3_-26',
 'X3_-25',
 'X3_-24',
 'X3_-23',
 'X3_-22',
 'X3_-21',
 'X3_-20',
 'X3_-19',
 'X3_-18',
 'X3_-17',
 'X3_-16',
 'X3_-15',
 'X3_-14',
 'X3_-13',
 'X3_-12',
 'X3_-11',
 'X3_-10',
 'X3_-9',
 'X3_-8',
 'X3_-7',
 'X3_-6',
 'X3_-5',
 'X3_-4',
 'X3_-3',
 'X3_-2',
 'X3_-1',
 'X3_0',
 'X3_1',
 'X3_2',
 'X3_3',
 'X3_4',
 'X3_5',
 'X3_6',
 'X3_7',
 'X3_8',
 'X3_9',
 'X3_10',
 'X3_11',
 'X3_12',
 'X3_13',
 'X3_14',
 'X3_15',
 'X3_16',
 'X3_17',
 'X3_18',
 'X3_19',
 'X3_20',
 'X3_21',
 'X3_22',
 'X3_23',
 'X3_24',
 'X3_25',
 'X3_26',
 'X3_27',
 'X3_28',
 'X3_29',
 'X3_30',
 'X3_31',
 'X3_32',
 'X3_33',
 'X3_34',
 'X3_35',
 'X3_36',
 'X3_37',
 'X3_38',
 'X3_39',
 'X3_40',
 'X3_41',
 'X3_42',
 'X3_43',
 'X3_44',
 'X3_45',
 'X3_46',
 'X3_47',
 'X3_48',
 'X3_49',
 'X3_50',
 'X3_51',
 'X3_52',
 'X3_53',
 'X3_54',
 'X3_55',
 'X3_56',
 'X3_57',
 'X3_58',
 'X3_59',
 'X3_60',
 'X3_61',
 'X3_62',
 'X3_63',
 'X3_64',
 'X3_65',
 'X3_66',
 'X3_67',
 'X3_68',
 'X3_69',
 'X3_70',
 'X3_71',
 'X3_72',
 'X3_73',
 'X3_74',
 'X3_75',
 'X3_76',
 'X3_77',
 'X3_78',
 'X3_79',
 'X3_80',
 'X3_81',
 'X3_82',
 'X3_83',
 'X3_84',
 'X3_85',
 'X3_86',
 'X3_87',
 'X3_88',
 'X3_89',
 'X3_90',
 'X3_91',
 'X3_92',
 'X3_93',
 'X3_94',
 'X3_95',
 'X3_96',
 'X3_97',
 'X3_98',
 'X3_99',
 'X3_100',
 'X3_101',
 'X3_102',
 'X3_103',
 'X3_104',
 'X3_105',
 'X3_106',
 'X3_107',
 'X3_108',
 'X3_109',
 'X3_110',
 'X3_111',
 'X3_112',
 'X3_113',
 'X3_114',
 'X3_115',
 'X3_116',
 'X3_117',
 'X3_118',
 'X3_119',
 'X3_120',
 'X3_121',
 'X3_122',
 'X3_123',
 'X3_124',
 'X3_125',
 'X3_126',
 'X3_127',
 'X3_128',
 'X3_129',
 'X3_130',
 'X3_131',
 'X3_132',
 'X3_133',
 'X3_134',
 'X3_135',
 'X3_136',
 'X3_137',
 'X3_138',
 'X3_139',
 'X3_140',
 'X3_141',
 'X3_142',
 'X3_143',
 'X3_144',
 'X3_145',
 'X3_146',
 'X3_147',
 'X3_148',
 'X3_149',
 'X3_150',
 'X3_151',
 'X3_152',
 'X3_153',
 'X3_154',
 'X3_155',
 'X3_156',
 'X3_157',
 'X3_158',
 'X3_159',
 'X3_160',
 'X3_161',
 'X3_162',
 'X3_163',
 'X3_164',
 'X3_165',
 'X3_166',
 'X3_167',
 'X3_168',
 'X3_169',
 'X3_170',
 'X3_171',
 'X3_172',
 'X3_173',
 'X3_174',
 'X3_175',
 'X3_176',
 'X3_177',
 'X3_178',
 'X3_179',
 'X3_180',
 'X3_181',
 'X3_183',
 'X3_184',
 'X3_186',
 'X3_187',
 'X3_188',
 'X3_189',
 'X3_190',
 'X3_191',
 'X3_192',
 'X3_193',
 'X3_194',
 'X3_195',
 'X3_196',
 'X3_197',
 'X3_198',
 'X3_199',
 'X3_201',
 'X3_202',
 'X3_203',
 'X3_204',
 'X3_205',
 'X3_206',
 'X3_207',
 'X3_208',
 'X3_209',
 'X3_210',
 'X3_211',
 'X3_212',
 'X3_213',
 'X3_214',
 'X3_215',
 'X3_216',
 'X3_217',
 'X3_218',
 'X3_219',
 'X3_220',
 'X3_221',
 'X3_222',
 'X3_223',
 'X3_224',
 'X3_225',
 'X3_226',
 'X3_227',
 'X3_228',
 'X3_229',
 'X3_230',
 'X3_231',
 'X3_233',
 'X3_234',
 'X3_235',
 'X3_236',
 'X3_237',
 'X3_238',
 'X3_239',
 'X3_241',
 'X3_243',
 'X3_244',
 'X3_245',
 'X3_246',
 'X3_247',
 'X3_250',
 'X3_252',
 'X3_254',
 'X3_255',
 'X3_256',
 'X3_257',
 'X3_259',
 'X3_260',
 'X3_262',
 'X3_265',
 'X3_266',
 'X3_267',
 'X3_268',
 'X3_269',
 'X3_270',
 'X3_272',
 'X3_273',
 'X3_274',
 'X3_275',
 'X3_277',
 'X3_279',
 'X3_281',
 'X3_282',
 'X3_283',
 'X3_284',
 'X3_285',
 'X3_286',
 'X3_287',]]
 
df.head(5)

scaler = StandardScaler()
scaler.fit(df.drop('y',axis = 1))

scaled_features = scaler.transform(df.drop('y',axis = 1))
df_feat = pd.DataFrame(scaled_features,columns = df.columns[:-1])
df_feat.head()

df.head(5)

col=df.describe().columns.to_list()
print(col)

sns.pairplot(df, hue="diagnosis")

X = df.iloc[:, 2:31].values 
Y = df.iloc[:, 1].values

from sklearn.model_selection import train_test_split
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.25, random_state = 0)

#Feature Scaling
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

from sklearn.neighbors import KNeighborsClassifier

from sklearn.ensemble import RandomForestClassifier

from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.svm import SVC
cv = KFold(n_splits=10, random_state=100, shuffle=True)
model = KNeighborsClassifier(n_neighbors=36)
scores = cross_val_score(model, X,Y, scoring='accuracy', cv=cv, n_jobs=-1)
print('Accuracy of KNN: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))
model = SVC(kernel='rbf')
scores = cross_val_score(model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)
print('Accuracy of SVC: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))
model=RandomForestClassifier(n_estimators =40,random_state=100)
scores = cross_val_score(model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)
print('Accuracy of RandomForest: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))
scores = cross_val_score(model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)
print('Accuracy of Logistic Regression: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))
scores = cross_val_score(model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)
print('Accuracy of deceision tree: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))

scores = cross_val_score(model, X, Y, scoring='accuracy', cv=cv, n_jobs=-1)
print('Accuracy of naive bayes: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))

"""**RF**"""

print("Training Random Forest")
rf = []
estimators = [10, 100, 200, 500, 1000]
for i in estimators:
    rf_classifier = RandomForestClassifier(n_estimators = i, random_state = 100)
    rf_classifier.fit(X_train, Y_train,)
    rf.append(rf_classifier.score(X_test, Y_test))

from sklearn.metrics import confusion_matrix, classification_report, log_loss
from sklearn import metrics
from sklearn import metrics

print('Confusion matrix of model',i , 'is :')
cm = confusion_matrix(Y_test, rf_classifier.predict(X_test))
TP = cm[0][0]
TN = cm[1][1]
FP = cm[1][0]
FN = cm[0][1]
print(cm)
print()
result1 = classification_report(Y_test, rf_classifier.predict(X_test))
print("Classification Report : ",)
print (result1)
print()
var = ((TP + TN)/(TP + TN + FP + FN)) *100
print('Testing accuracy : ',var)
print('Sensitivity : ', TP/(TP+FN))
print('Specificity : ', TN/(TN+FP))
print('false positive rate : ', FP/(FP+TN))
print('false negative rate : ', FN/(FN+TP))
print('Negative Predictive Value : ', TN/(TN+FN))
print('False Discovery rate : ', FP/(TP+FP))
print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, rf_classifier.predict(X_test)))
print('Mean Squared Error:', metrics.mean_squared_error(Y_test, rf_classifier.predict(X_test)))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, rf_classifier.predict(X_test))))
print('Log_Loss:', metrics.log_loss(Y_test, rf_classifier.predict(X_test)))

plt.bar([i for i in range(len(estimators))], rf)
for i in range(len(estimators)):
    plt.text(i, round(rf[i],2), round(rf[i]*100,2))
plt.xticks(ticks = [i for i in range(len(estimators))], labels = [str(estimator) for estimator in estimators])
plt.xlabel('Estimator')
plt.ylabel('Score')
plt.title('Random Forest')

"""**KNN**"""

print("Training K-Nearest Neighbors")
knn = []
for i in range(1, 21):
    knn_classifier = KNeighborsClassifier(n_neighbors = i)
    knn_classifier.fit(X_train, Y_train)
    knn.append(knn_classifier.score(X_test, Y_test))

print('Confusion matrix of model',i , 'is :')
cm = confusion_matrix(Y_test, knn_classifier.predict(X_test))
TP = cm[0][0]
TN = cm[1][1]
FP = cm[1][0]
FN = cm[0][1]
print(cm)
print()
result1 = classification_report(Y_test,  knn_classifier.predict(X_test))
print("Classification Report : ",)
print (result1)
print()
var = ((TP + TN)/(TP + TN + FP + FN)) *100
print('Testing accuracy : ',var)
print('Sensitivity : ', TP/(TP+FN))
print('Specificity : ', TN/(TN+FP))
print('false positive rate : ', FP/(FP+TN))
print('false negative rate : ', FN/(FN+TP))
print('Negative Predictive Value : ', TN/(TN+FN))
print('False Discovery rate : ', FP/(TP+FP))
print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test,  knn_classifier.predict(X_test)))
print('Mean Squared Error:', metrics.mean_squared_error(Y_test,  knn_classifier.predict(X_test)))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test,  knn_classifier.predict(X_test))))
print('Log_Loss:', metrics.log_loss(Y_test,  knn_classifier.predict(X_test)))

plt.plot([i for i in range(1, 21)], knn)
for i in range(1, 21):
    plt.text(i, knn[i - 1], (i, round(knn[i - 1]*100, 2)))
plt.xticks([i for i in range(1, 21)])
plt.xlabel('Neighbors')
plt.ylabel('Score')
plt.title('K-Nearest Neighbors')



"""**SVM**"""

svc = []
activators = ['poly', 'sigmoid', 'linear', 'rbf']
for i in range(len(activators)):
    SVclassifier = SVC(kernel = activators[i])
    SVclassifier.fit(X_train, Y_train)
    svc.append(SVclassifier.score(X_test,Y_test ))

from sklearn.metrics import confusion_matrix, classification_report, log_loss
from sklearn import metrics
from sklearn import metrics

print('Confusion matrix of model',i , 'is :')
cm = confusion_matrix(Y_test,  SVclassifier.predict(X_test))
TP = cm[0][0]
TN = cm[1][1]
FP = cm[1][0]
FN = cm[0][1]
print(cm)
print()
result1 = classification_report(Y_test ,  SVclassifier.predict(X_test))
print("Classification Report : ",)
print (result1)
print()
var = ((TP + TN)/(TP + TN + FP + FN)) *100
print('Testing accuracy : ',var)
print('Sensitivity : ', TP/(TP+FN))
print('Specificity : ', TN/(TN+FP))
print('false positive rate : ', FP/(FP+TN))
print('false negative rate : ', FN/(FN+TP))
print('Negative Predictive Value : ', TN/(TN+FN))
print('False Discovery rate : ', FP/(TP+FP))
print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, SVclassifier.predict(X_test)))
print('Mean Squared Error:', metrics.mean_squared_error(Y_test,  SVclassifier.predict(X_test)))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test,  SVclassifier.predict(X_test))))
print('Log_Loss:', metrics.log_loss(Y_test,  SVclassifier.predict(X_test)))

plt.bar(activators, svc)
for i in range(len(activators)):
    plt.text(i, round(svc[i],2), round(svc[i]*100,2))
plt.xlabel('Activators')
plt.ylabel('Score')
plt.title('Support Vector Classifier')

"""**LR**"""

from sklearn.linear_model import LogisticRegression

lr = LogisticRegression(random_state=0)
lr.fit(X_train,Y_train)
s = np.mean(cross_val_score(lr,X_train,Y_train,scoring='roc_auc',cv=5))
print('The accuracy score for Logistic Regression is: ', s*100)

from sklearn.tree import DecisionTreeClassifier

tree = DecisionTreeClassifier(criterion = 'entropy', max_depth=1, random_state = 42)
tree.fit(X_train,Y_train)

from sklearn.metrics import confusion_matrix, classification_report, log_loss
from sklearn import metrics
from sklearn import metrics

print('Confusion matrix of model',i , 'is :')
cm = confusion_matrix(Y_test, tree.predict(X_test))
TP = cm[0][0]
TN = cm[1][1]
FP = cm[0][1]
FN = cm[1][0]
print(cm)
print()
result1 = classification_report(Y_test, tree.predict(X_test))
print("Classification Report : ",)
print (result1)
print()
var = ((TP + TN)/(TP + TN + FP + FN)) *100
print('Testing accuracy : ',var)
print('Sensitivity : ', TP/(TP+FN))
print('Specificity : ', TN/(TN+FP))
print('false positive rate : ', FP/(FP+TN))
print('false negative rate : ', FN/(FN+TP))
print('Negative Predictive Value : ', TN/(TN+FN))
print('False Discovery rate : ', FP/(TP+FP))
print('Mean Absolute Error:', metrics.mean_absolute_error(Y_test, tree.predict(X_test)))
print('Mean Squared Error:', metrics.mean_squared_error(Y_test, tree.predict(X_test)))
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(Y_test, tree.predict(X_test))))
print('Log_Loss:', metrics.log_loss(Y_test, tree.predict(X_test)))

"""**naive bayes**"""

from sklearn.naive_bayes import GaussianNB

gaussian = GaussianNB()
gaussian.fit(X_train, Y_train)

Y_pred = gaussian.predict(X_test)

acc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)
print(round(acc_gaussian,2,), "%")